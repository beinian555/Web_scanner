# core/crawler/spider_engine.py
import json
import time
from urllib.parse import urljoin, urlparse
from playwright.sync_api import sync_playwright
import redis

class SpiderEngine:
    def __init__(self, start_url):
        self.start_url = start_url.rstrip('/')
        self.domain = urlparse(start_url).netloc
        # è¿æ¥ Redis å¹¶è®¾ç½® decode_responses=True æ–¹ä¾¿è¯»å–å­—ç¬¦ä¸²
        self.r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
        self.queue_key = 'vuln_scan_queue'
        self.visited_key = f'visited_urls:{self.domain}'

    def extract_links(self, page):
        """ä»é¡µé¢ä¸­æå–æ‰€æœ‰å±äºæœ¬ç«™çš„é“¾æ¥"""
        # è·å–æ‰€æœ‰ a æ ‡ç­¾çš„ href å±æ€§
        links = page.eval_on_selector_all("a[href]", "elements => elements.map(el => el.href)")
        valid_links = []
        for link in links:
            # è½¬åŒ–ä¸ºç»å¯¹è·¯å¾„å¹¶å»æ‰æœ«å°¾æ–œæ 
            full_url = urljoin(page.url, link).split('#')[0].rstrip('/')
            # åªè¦æ˜¯åŒåŸŸåçš„é“¾æ¥å°±è®¤ä¸ºæœ‰æ•ˆ
            if urlparse(full_url).netloc == self.domain:
                valid_links.append(full_url)
        return list(set(valid_links))

    def run(self):
        """è¿è¡Œçˆ¬è™«ä¸»é€»è¾‘"""
        # --- æ ¸å¿ƒï¼šæ¯æ¬¡å¼€å§‹å‰æ¸…ç©º Redisï¼Œé˜²æ­¢â€œé˜Ÿåˆ—å·²ç©ºâ€ ---
        print("ğŸ§¹ æ­£åœ¨åˆå§‹åŒ–æ‰«æç¯å¢ƒï¼Œæ¸…ç©ºæ—§ç¼“å­˜...")
        self.r.flushall() 

        with sync_playwright() as p:
            # æ¼”ç¤ºæ—¶å»ºè®®å¼€å¯ headless=False å¯ä»¥äº²çœ¼çœ‹åˆ°æµè§ˆå™¨åœ¨çˆ¬
            browser = p.chromium.launch(headless=True)
            context = browser.new_context()
            page = context.new_page()

            print(f"ğŸš€ çˆ¬è™«å¯åŠ¨ï¼Œç›®æ ‡: {self.start_url}")
            
            # 1. èµ·å§‹ URL å…¥é˜Ÿ
            self.r.lpush(self.queue_key, json.dumps({"url": self.start_url, "method": "GET"}))

            while True:
                # 2. ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
                task_raw = self.r.rpop(self.queue_key)
                if not task_raw:
                    print("âœ… æ‰€æœ‰é¡µé¢å·²çˆ¬å–å®Œæ¯•ã€‚")
                    break
                
                task = json.loads(task_raw)
                current_url = task['url']

                # 3. å»é‡æ£€æŸ¥
                if self.r.sismember(self.visited_key, current_url):
                    continue

                print(f"ğŸ” æ­£åœ¨çˆ¬å–: {current_url}")
                
                try:
                    page.goto(current_url, wait_until="networkidle", timeout=10000)
                    self.r.sadd(self.visited_key, current_url)

                    # 4. æå–æ–°é“¾æ¥å¹¶å…¥é˜Ÿ
                    new_links = self.extract_links(page)
                    for link in new_links:
                        if not self.r.sismember(self.visited_key, link):
                            self.r.lpush(self.queue_key, json.dumps({"url": link, "method": "GET"}))
                    
                    print(f"  â””â”€ å‘ç°æ–°é“¾æ¥: {len(new_links)} ä¸ª")
                except Exception as e:
                    print(f"  âŒ çˆ¬å–å¤±è´¥ {current_url}: {e}")

            browser.close()